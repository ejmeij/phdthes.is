<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Relevance</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- mathml,index=2,3,html --> 
<meta name="src" content="thesis-emeij.tex"> 
<meta name="date" content="2010-11-28 20:26:00"> 
<link rel="stylesheet" type="text/css" href="thesis-emeij.css"> 
</head><body 
>
   <table cellspacing="5"><tr><td class="clinks"><a 
href="thesis-emeijch3.html#thesis-emeijse14.html" >Up</a></td><td class="clinks"><a 
href="thesis-emeijse15.html" >Next</a></td><td class="clinks"><a 
href="#tailthesis-emeijse14.html">Tail</a></td></tr></table><h3 class="sectionHead"><span class="titlemark">3.1    </span> <a 
 id="x20-340001"></a>Relevance</h3>
<!--l. 1314--><p class="noindent" >Central to the evaluation of <a 
href="thesis-emeijli3.html#IR">IR</a> systems is the notion of relevance. Relevance of a piece
of information (be it a web page, document, passage, or anything else) is measured
against an information need of some user. Contextual factors such as presentation or
document style aside&#x00A0;[<a 
href="thesis-emeijli2.html#XASIST:1990:hewins">133</a>], determining a topical definition of an information need is
subject to various user-based parameters&#x00A0;[<a 
href="thesis-emeijli2.html#XECDL:2009:Kamps">159</a>]. For example, different users may have
different backgrounds, their understanding of the topic might change as they browse
through a result list, or they may aim to solve different tasks. Objectively determining
relevance of a piece of information to an information need is difficult to operationalize.
Cool <span 
class="bchri8t-">et</span><span 
class="bchri8t-">&#x00A0;al.</span>&#x00A0;[<a 
href="thesis-emeijli2.html#XNOM:1993:cool">78</a>], for example, studied the real life tasks of writing an essay and found
that characteristics other than topical relevance affect a person&#x2019;s evaluation
of a document&#x2019;s usefulness. This complexity of relevance as an evaluation
criterion has been recognized already by Saracevic&#x00A0;[<a 
href="thesis-emeijli2.html#XJASIS:1975:saracevic">279</a>] and is still pertinent
today.
                                                                   
                                                                   
<!--l. 1316--><p class="indent" >   Cooper&#x00A0;[<a 
href="thesis-emeijli2.html#XJASIS:1973:cooper">79</a>] posits that any valid measure of IR system performance must be
derived from the goal of such a system. Since the goal is to satisfy the information need
of a user, a measure of utility to the user is required. Cooper concludes that user
satisfaction with the results generated by a system is the optimal measure of
performance. These intuitions provide the basis for the user-based approach to <a 
href="thesis-emeijli3.html#IR">IR</a>
system evaluation. According to this view, systems should be evaluated on how well
they provide the information needed by a user. And, in turn, the best judge of
this performance is the person who is going to use the information. Despite
criticisms&#x00A0;[<a 
href="thesis-emeijli2.html#XJASIS:1976:soergl">289</a>], researchers committed to a user-centered model of system
evaluation.
<!--l. 1318--><p class="indent" >   The Cranfield experiments sidestepped any issues pertaining to relevance&#x00A0;[<a 
href="thesis-emeijli2.html#Xcranfield:1970:cleverdon">74</a>,&#x00A0;<a 
href="thesis-emeijli2.html#Xcranfield:1966:cleverdon">75</a>,&#x00A0;<a 
href="thesis-emeijli2.html#XJIS:2008:Robertson">260</a>].
In Cranfield I, queries were generated from documents and the goal was to retrieve the
document each query was generated from. As such, there was only a single relevant
document to be retrieved for each query. In Cranfield II, queries were generated in the
same way, but each document was now manually judged for relevance. In a recent
study, Kelly <span 
class="bchri8t-">et</span><span 
class="bchri8t-">&#x00A0;al.</span>&#x00A0;[<a 
href="thesis-emeijli2.html#XTOIS:2010:kelly">163</a>] report on the results of a user study. They find that there exists
linear relationships between the users&#x2019; perception of system performance and the
position of relevant documents in a search results list as well as the total number of
retrieved relevant documents; the number of relevant documents retrieved
was a stronger predictor of the users&#x2019; evaluation ratings. In the next section
we introduce the common methodology associated with the evaluation of <a 
href="thesis-emeijli3.html#IR">IR</a>
systems.
                                                                   
                                                                   
<!--l. 1322--><p class="indent" >   <table cellspacing="5"><tr><td class="clinks"><a 
href="thesis-emeijch3.html#thesis-emeijse14.html" >Up</a></td><td class="clinks"><a 
href="thesis-emeijse15.html" >Next</a></td><td class="clinks"><a 
href="thesis-emeijse14.html" >Front</a></td></tr></table><a 
 id="tailthesis-emeijse14.html"></a> 
</body></html> 
